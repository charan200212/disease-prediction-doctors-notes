
# Disease Prediction from Doctorâ€™s Notes (NLP + ML)

An endâ€‘toâ€‘end, **teachingâ€‘oriented** project that turns unstructured clinical notes into disease predictions using classical NLP (TFâ€‘IDF) + ML (Logistic Regression / SVM), integrated with **MySQL** for data storage and **Git/GitHub** workflows.

## ðŸ“¦ Whatâ€™s Inside
- `data/synthetic_notes.csv` â€” a small synthetic dataset to start immediately.
- `sql/schema.sql` â€” MySQL tables for notes and predictions.
- `src/preprocess.py` â€” text cleaning, tokenization, lemmatization.
- `src/db.py` â€” MySQL helpers (create tables, insert/fetch notes, save predictions).
- `src/train.py` â€” train baseline models (TFâ€‘IDF + LogisticRegression/SVM).
- `src/evaluate.py` â€” evaluate models with precision/recall/F1.
- `src/pipeline.py` â€” endâ€‘toâ€‘end: fetch -> preprocess -> predict -> store.
- `src/cli.py` â€” quick CLI for adâ€‘hoc predictions.
- `src/config.py` â€” central place for environment variables.
- `src/prepare_nltk.py` â€” downloads NLTK resources once.
- `models/` â€” saved vectorizer + model (after training).
- `reports/` â€” evaluation reports saved here.

> **Tech**: Linux, Python, Git/GitHub, MySQL, pandas, scikitâ€‘learn, NLTK, joblib.

---

## ðŸ§° Setup

1) **Python env**
```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
python -m src.prepare_nltk  # download NLTK data once
```

2) **MySQL**
- Start a local MySQL server.
- Create a DB, e.g. `clinical_nlp`.
- Update **env vars** (see below) or edit `src/config.py`.

3) **Environment variables**
Create a `.env` file (or export in shell):
```
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=yourpassword
DB_NAME=clinical_nlp
# Optional: set to 'sqlite' for local/offline testing
DB_BACKEND=mysql
SQLITE_PATH=./local.db
```

4) **Load schema**
```bash
mysql -u $DB_USER -p$DB_PASSWORD -h $DB_HOST -P $DB_PORT $DB_NAME < sql/schema.sql
```

5) **Load synthetic data into MySQL**
```bash
python -m src.db --load_csv data/synthetic_notes.csv
```

6) **Train & Evaluate**
```bash
python -m src.train
python -m src.evaluate
```

7) **Endâ€‘toâ€‘End Pipeline (DB -> Predict -> Store)**
```bash
python -m src.pipeline
```

8) **Adâ€‘hoc Prediction from CLI**
```bash
python -m src.cli --text "Patient reports fever, cough, body aches for 3 days."
```

---

## ðŸ§ª Dataset

`data/synthetic_notes.csv` has three columns:
- `note_id` â€” integer ID
- `note_text` â€” clinical note (free text)
- `label` â€” target diagnosis (class)

This is a **toy** dataset for learning. Replace with your curated dataset once ready (ensure the same columns).

---

## ðŸ§  Models

- Baseline: **TFâ€‘IDF + Logistic Regression**
- Alternative: **Linear SVM (SGDClassifier hinge)**

Saved to `models/vectorizer.joblib` and `models/model.joblib`.

---

## ðŸ—ƒï¸ MySQL Schema (summary)

- `notes(note_id INT PK, note_text TEXT, label VARCHAR(64))`
- `predictions(pred_id INT PK AUTO, note_id INT FK, predicted_label VARCHAR(64), proba JSON, created_at TIMESTAMP)`

---

## ðŸ” Typical Workflow (Phaseâ€‘2 Project Days)

1. **Day 6â€“7**: Ingest/clean/EDA, load MySQL.
2. **Day 8**: Build baseline TFâ€‘IDF + LR model.
3. **Day 9â€“10**: Improve model (SVM, tuning), log results.
4. **Day 11**: Wire pipeline DB -> preprocess -> predict -> store.
5. **Day 12â€“13**: Tests, CLI, docs, error handling.
6. **Day 14**: Final demo, README polish, repo hygiene.

---

## ðŸ“œ License
MIT â€” free to use for learning and internal projects.
